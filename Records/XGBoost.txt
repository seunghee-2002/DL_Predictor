XGBoost 기반 모델 학습 기록 보고
1. 초기 접근 시도 및 한계
입력 구조: (주문당 최대 145개 제품 × 최근 5회 주문 × 제품 임베딩 16) 형태의 3D 텐서를 직접 XGBoost에 입력하고자 함.

문제점
- XGBoost는 2D 입력만 허용되며, 3D 구조를 직접 처리할 수 없음.
멀티라벨 예측을 위해 약 50,000개 제품을 각각 이진 분류하는 OVR 구조(145x5x16)를 그대로 적용하면 모델 수가 과도하게 많아짐.
학습 속도 및 메모리 사용량 문제로 중단.

2. 구조 변경 방향성
원래 DL 모델이 CNN으로 (145, 64)주문을 128차원으로 요약하듯이 미리 고차원 구조를 1D 벡터로 요약하여 2D 입력으로 변경.
전체 제품 중 상위 K개만 후보로 제한하는 Top-K 후보 기반 예측으로 모델 수 감소.
결국 304,000명 사용자 × 128차원 벡터 입력, 다중 레이블(Top-K) OVR 분류 구조로 설계.

3. 전처리 방식 요약
미리 학습된 16차원 제품 임베딩(ev_final16.npy) 사용
- 각 주문 내 제품 임베딩을 W @ ev + b (1-layer MLP) 적용 후, Max Pooling
- 최근 5회 주문을 시계열 가중합 (λ=0.8)으로 통합해 유저 벡터(seq_vec128.npy) 생성
- 유저 벡터와 제품 벡터 간 내적 유사도 기반으로 상위 K개 제품 추출 (K=300)
- 실제 주문(product_id)이 후보 안에 존재하면 1, 없으면 0 → CSR 라벨 행렬(Y_topk.npz) 생성

4. XGBoost 학습 설정
모델 구조: One-vs-Rest, 총 300개 이진 분류기
입력 X: seq_vec128.npy (304k × 128) → X_128.npz로 CSR 저장
라벨 Y: Top-K 후보 300개에 대한 다중 레이블 → Y_topk.npz
트리 설정: tree_method=gpu_hist, predictor=gpu_predictor, max_depth=6, learning_rate=0.1
클래스 불균형 보정: scale_pos_weight = (#negative / #positive) per class
Early Stopping	patience = 30
평가 지표: logloss (각 클래스), 최종적으로 F1-micro 전체 평균


5. 학습 결과
F1-micro (threshold=0.4 기준) : 0.0119

6. 요약
DL model과 비슷한 구조로 진행
   1. (145, 5, 64) 주문 시퀀스를 (사용자, 128)로 미리 전처리 (CNN)
   2. 전처리한 데이터를 활용해 XGBoost 학습 (GRU + FC)
   - ML model 역시 pos_weight를 활용했음 (미활용시 성능:f1micro=0.0021) (사진 첨부)

모델 수를 300개로 제한하여 학습 가능 시간 내 처리 가능했으며, 추후 DL 모델 대비 baseline으로 활용 가능.

