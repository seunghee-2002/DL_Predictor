CNN출력 채널, CNN 커널 크기, GRU 은닉 상태 차원,
GRU 계층 수, 드롭아웃 비율, 예측 헤드 중간 차원,
학습률, 배치 크기,
구매로 판단할 확률 임계값, pos_weight 사용 여부

1

128, 3, 256,
1, 0.1, 128
0.001, 32
0.1, False

학습 손실: 0.0013, 검증 손실: 0.0013, 검증 지표: [F1_micro_overall: 0.0366, F1_macro: 0.0003, Precision@10: 0.6008, Recall@10: 0.0189, Precision@20: 0.6008, Recall@20: 0.0189]

2

256, 5, 512,
2, 0.1, 256
0.001, 32
0.1, False

최종 테스트 세트 손실: 0.0013, 테스트 지표: [F1_micro_overall: 0.0288, F1_macro: 0.0001, Precision@10: 0.6199, Recall@10: 0.0147, Precision@20: 0.6199, Recall@20: 0.0147]    

3

256, 5, 512,
2, 0.1, 256
0.001, 32
0.1, True

최종 테스트 세트 손실: 0.5924, 테스트 지표: [F1_micro_overall: 0.0032, F1_macro: 0.0005, Precision@10: 0.0016, Recall@10: 0.8878, Precision@20: 0.0016, Recall@20: 0.8878]

4

256, 5, 512,
2, 0.1, 256
0.001, 32
0.5, True

최종 테스트 세트 손실: 0.5924, 테스트 지표: [F1_micro_overall: 0.0013, F1_macro: 0.0005, Precision@10: 0.0006, Recall@10: 0.9741, Precision@20: 0.0006, Recall@20: 0.9741]

threshold 실험

0.1
최종 테스트 세트 손실: 0.2456, 테스트 지표: [F1_micro_overall: 0.0013, F1_macro: 0.0005, Precision@10: 0.0006, Recall@10: 0.9741, Precision@20: 0.0006, Recall@20: 0.9741]
0.15
최종 테스트 세트 손실: 0.2456, 테스트 지표: [F1_micro_overall: 0.0013, F1_macro: 0.0005, Precision@10: 0.0006, Recall@10: 0.9741, Precision@20: 0.0006, Recall@20: 0.9741]

cnn_kernelsize = 5
최종 테스트 세트 손실: 0.0091, 테스트 지표: [F1_micro_overall: 0.1130, F1_macro: 0.0004, Precision@10: 0.1351, Recall@10: 0.0971, Precision@20: 0.1351, Recall@20: 0.0971] 
cnn_kernelsize = 4
모델 가중치 로드 완료: ./models\cnn_gru_cnn.pth
최종 테스트 세트 손실: 0.0089, 테스트 지표: [F1_micro_overall: 0.1297, F1_macro: 0.0009, Precision@10: 0.1386, Recall@10: 0.1219, Precision@20: 0.1386, Recall@20: 0.1219]

[pw=5, th=0.10] Prec=0.0411 Rec=0.1669 F1=0.0659
[pw=5, th=0.20] Prec=0.0825 Rec=0.0783 F1=0.0803
[pw=5, th=0.30] Prec=0.1218 Rec=0.0460 F1=0.0668
[pw=5, th=0.40] Prec=0.1571 Rec=0.0278 F1=0.0472
[pw=5, th=0.50] Prec=0.1937 Rec=0.0119 F1=0.0223
[pw=5, th=0.60] Prec=0.0000 Rec=0.0000 F1=0.0000
[pw=5, th=0.70] Prec=0.0000 Rec=0.0000 F1=0.0000

[pw=10, th=0.10] Prec=0.0322 Rec=0.2092 F1=0.0559
[pw=10, th=0.20] Prec=0.0611 Rec=0.1124 F1=0.0792
[pw=10, th=0.30] Prec=0.1010 Rec=0.0625 F1=0.0772
[pw=10, th=0.40] Prec=0.1382 Rec=0.0393 F1=0.0612
[pw=10, th=0.50] Prec=0.1621 Rec=0.0205 F1=0.0364
[pw=10, th=0.60] Prec=0.2333 Rec=0.0049 F1=0.0096
[pw=10, th=0.70] Prec=0.0000 Rec=0.0000 F1=0.0000

[pw=15, th=0.10] Prec=0.0178 Rec=0.3205 F1=0.0338
[pw=15, th=0.20] Prec=0.0342 Rec=0.2036 F1=0.0585
[pw=15, th=0.30] Prec=0.0520 Rec=0.1389 F1=0.0757
[pw=15, th=0.40] Prec=0.0725 Rec=0.0971 F1=0.0830
[pw=15, th=0.50] Prec=0.0998 Rec=0.0650 F1=0.0787
[pw=15, th=0.60] Prec=0.1346 Rec=0.0420 F1=0.0640
[pw=15, th=0.70] Prec=0.1847 Rec=0.0208 F1=0.0374

[pw=20, th=0.10] Prec=0.0147 Rec=0.3591 F1=0.0283
[pw=20, th=0.20] Prec=0.0280 Rec=0.2371 F1=0.0501
[pw=20, th=0.30] Prec=0.0431 Rec=0.1645 F1=0.0683
[pw=20, th=0.40] Prec=0.0628 Rec=0.1126 F1=0.0807
[pw=20, th=0.50] Prec=0.0876 Rec=0.0766 F1=0.0817
[pw=20, th=0.60] Prec=0.1152 Rec=0.0529 F1=0.0725
[pw=20, th=0.70] Prec=0.1505 Rec=0.0335 F1=0.0548

[pw=25, th=0.10] Prec=0.0109 Rec=0.4212 F1=0.0213
[pw=25, th=0.20] Prec=0.0213 Rec=0.2865 F1=0.0397
[pw=25, th=0.30] Prec=0.0323 Rec=0.2127 F1=0.0560
[pw=25, th=0.40] Prec=0.0455 Rec=0.1584 F1=0.0707
[pw=25, th=0.50] Prec=0.0636 Rec=0.1102 F1=0.0807
[pw=25, th=0.60] Prec=0.0866 Rec=0.0765 F1=0.0812
[pw=25, th=0.70] Prec=0.1217 Rec=0.0491 F1=0.0700

[pw=30, th=0.10] Prec=0.0105 Rec=0.4296 F1=0.0204
[pw=30, th=0.20] Prec=0.0200 Rec=0.2991 F1=0.0374
[pw=30, th=0.30] Prec=0.0298 Rec=0.2276 F1=0.0527
[pw=30, th=0.40] Prec=0.0421 Rec=0.1693 F1=0.0674
[pw=30, th=0.50] Prec=0.0585 Rec=0.1213 F1=0.0789
[pw=30, th=0.60] Prec=0.0790 Rec=0.0860 F1=0.0823
[pw=30, th=0.70] Prec=0.1099 Rec=0.0571 F1=0.0751

[pw=35, th=0.10] Prec=0.0093 Rec=0.4577 F1=0.0182
[pw=35, th=0.20] Prec=0.0183 Rec=0.3179 F1=0.0346
[pw=35, th=0.30] Prec=0.0281 Rec=0.2390 F1=0.0502
[pw=35, th=0.40] Prec=0.0391 Rec=0.1805 F1=0.0643
[pw=35, th=0.50] Prec=0.0553 Rec=0.1285 F1=0.0773
[pw=35, th=0.60] Prec=0.0788 Rec=0.0860 F1=0.0822
[pw=35, th=0.70] Prec=0.1157 Rec=0.0532 F1=0.0729

[pw=40, th=0.10] Prec=0.0107 Rec=0.4185 F1=0.0208
[pw=40, th=0.20] Prec=0.0214 Rec=0.2799 F1=0.0398
[pw=40, th=0.30] Prec=0.0328 Rec=0.2055 F1=0.0566
[pw=40, th=0.40] Prec=0.0455 Rec=0.1504 F1=0.0699
[pw=40, th=0.50] Prec=0.0616 Rec=0.1071 F1=0.0782
[pw=40, th=0.60] Prec=0.0884 Rec=0.0697 F1=0.0779
[pw=40, th=0.70] Prec=0.1258 Rec=0.0436 F1=0.0647

[pw=45, th=0.10] Prec=0.0079 Rec=0.4989 F1=0.0156
[pw=45, th=0.20] Prec=0.0149 Rec=0.3625 F1=0.0286
[pw=45, th=0.30] Prec=0.0226 Rec=0.2798 F1=0.0418
[pw=45, th=0.40] Prec=0.0320 Rec=0.2180 F1=0.0558
[pw=45, th=0.50] Prec=0.0437 Rec=0.1672 F1=0.0693
[pw=45, th=0.60] Prec=0.0596 Rec=0.1218 F1=0.0800
[pw=45, th=0.70] Prec=0.0843 Rec=0.0816 F1=0.0829

[pw=50, th=0.10] Prec=0.0068 Rec=0.5334 F1=0.0135
[pw=50, th=0.20] Prec=0.0130 Rec=0.3917 F1=0.0251
[pw=50, th=0.30] Prec=0.0198 Rec=0.3069 F1=0.0373
[pw=50, th=0.40] Prec=0.0282 Rec=0.2415 F1=0.0506
[pw=50, th=0.50] Prec=0.0389 Rec=0.1869 F1=0.0644
[pw=50, th=0.60] Prec=0.0539 Rec=0.1362 F1=0.0772
[pw=50, th=0.70] Prec=0.0783 Rec=0.0899 F1=0.0837

[pw=100, th=0.10] Prec=0.0040 Rec=0.6661 F1=0.0079
[pw=100, th=0.20] Prec=0.0075 Rec=0.5224 F1=0.0147
[pw=100, th=0.30] Prec=0.0116 Rec=0.4229 F1=0.0226
[pw=100, th=0.40] Prec=0.0168 Rec=0.3456 F1=0.0321
[pw=100, th=0.50] Prec=0.0236 Rec=0.2797 F1=0.0435
[pw=100, th=0.60] Prec=0.0326 Rec=0.2221 F1=0.0568
[pw=100, th=0.70] Prec=0.0464 Rec=0.1637 F1=0.0723

[GridSearch] AdaptiveMaxPool1d output_size=8
output_size=8 → Precision=0.0805, Recall=0.1321, F1=0.1001

[GridSearch] AdaptiveMaxPool1d output_size=16
output_size=16 → Precision=0.0839, Recall=0.1180, F1=0.0981

[GridSearch] AdaptiveMaxPool1d output_size=32
output_size=32 → Precision=0.0837, Recall=0.1078, F1=0.0942

최고 F1=0.1001 → output_size=8

CNNGRU_1
cnn출력채널 128, cnn커널 크기 4, cnn pool 크기 8, gru 은닉상태 차원 256, gru 레이어 수 2, Dropout Rate 0.1, FC 중간 차원 256, 학습률 0.0001, 배치 크기 64, threshold 0.4, pos_weight 15 epoch 100

CNNGRU_2
cnn출력채널 128, cnn커널 크기 4, cnn pool 크기 8, gru 은닉상태 차원 256, gru 레이어 수 2, Dropout Rate 0.2, FC 중간 차원 256, 학습률 0.001, 배치 크기 64, threshold 0.4, pos_weight 15 epoch 100

CNNGRU_3
cnn출력채널 256, cnn커널 크기 4, cnn pool 크기 8, gru 은닉상태 차원 256, gru 레이어 수 2, Dropout Rate 0.1, FC 중간 차원 256, 학습률 0.0005, 배치 크기 64, threshold 0.4, pos_weight 15 epoch 100

CNN1 (주문 정보  X)
cnn출력채널 256, cnn커널 크기 5, cnn pool 크기 16, gru 은닉상태 차원 256, gru 레이어 수 2, Dropout Rate 0.2, FC 중간 차원 256, 학습률 0.0005, 배치 크기 64, threshold 0.4, pos_weight 15 epoch 100

CNN2
cnn출력채널 512, cnn커널 크기 4, cnn pool 크기 8, gru 은닉상태 차원 256, gru 레이어 수 2, Dropout Rate 0.2, FC 중간 차원 256, 학습률 0.0001, 배치 크기 64, threshold 0.4, pos_weight 15 epoch 100
